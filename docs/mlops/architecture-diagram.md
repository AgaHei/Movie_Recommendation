# CineMatch Architecture Overview

## ğŸ—ï¸ System Architecture

This document provides a comprehensive overview of the CineMatch MLOps architecture, detailing all components, their interactions, and the data flow through the entire system.

---

## ğŸ“‹ Table of Contents

1. [High-Level Architecture](#high-level-architecture)
2. [Component Details](#component-details)
3. [Data Flow](#data-flow)
4. [Integration Points](#integration-points)
5. [Tech Stack](#tech-stack)
6. [Deployment Architecture](#deployment-architecture)
7. [Design Decisions](#design-decisions)

---

## ğŸ¯ High-Level Architecture

### System Overview Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CINEMATCH MLOPS PIPELINE                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 1: DATA INGESTION & STORAGE                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚  MovieLens   â”‚                                                   â”‚
â”‚  â”‚   Dataset    â”‚                                                   â”‚
â”‚  â”‚   (25M)      â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚         â”‚                                                           â”‚
â”‚         â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Local Preprocessing     â”‚         â”‚   Buffer Batches     â”‚    â”‚
â”‚  â”‚  - Data cleaning         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   - batch_w1.parquet â”‚    â”‚
â”‚  â”‚  - Sampling (1M)         â”‚         â”‚   - batch_w3.parquet â”‚    â”‚
â”‚  â”‚  - Train/test split      â”‚         â”‚   - batch_w5.parquet â”‚    â”‚
â”‚  â”‚  - Feature engineering   â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚                â”‚
â”‚             â”‚                                      â”‚                â”‚
â”‚             â–¼                                      â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                â”‚
â”‚  â”‚         NEON POSTGRESQL                       â”‚ â”‚                â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚                â”‚
â”‚  â”‚  â”‚ ratings (1M)     â”‚ movies (60k)         â”‚ â”‚ â”‚                â”‚
â”‚  â”‚  â”‚ - train: 700k    â”‚ - title              â”‚ â”‚ â”‚                â”‚
â”‚  â”‚  â”‚ - test: 300k     â”‚ - genres             â”‚ â”‚ â”‚                â”‚
â”‚  â”‚  â”‚                  â”‚ - features           â”‚ â”‚ â”‚                â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚                â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚                â”‚
â”‚  â”‚  â”‚ ratings_buffer (0-1.5M)                 â”‚â—€â”˜ â”‚                â”‚
â”‚  â”‚  â”‚ - batch_id       â”‚ - ingested_at        â”‚   â”‚                â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                â”‚
â”‚  â”‚  â”‚ Metadata Tables                         â”‚   â”‚                â”‚
â”‚  â”‚  â”‚ - ingestion_metadata                    â”‚   â”‚                â”‚
â”‚  â”‚  â”‚ - model_metrics                         â”‚   â”‚                â”‚
â”‚  â”‚  â”‚ - drift_alerts                          â”‚   â”‚                â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                          â”‚                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: ORCHESTRATION & MONITORING                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          â”‚                                           â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚            â”‚   APACHE AIRFLOW (Docker)    â”‚                         â”‚
â”‚            â”‚   Local Development          â”‚                         â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                          â”‚                                           â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚         â”‚                â”‚                â”‚                         â”‚
â”‚         â–¼                â–¼                â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ DAG 1:      â”‚  â”‚ DAG 2:      â”‚  â”‚ DAG 3:      â”‚                â”‚
â”‚  â”‚ Buffer      â”‚  â”‚ Drift       â”‚  â”‚ Training    â”‚                â”‚
â”‚  â”‚ Ingestion   â”‚  â”‚ Monitoring  â”‚  â”‚ Trigger     â”‚                â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚                â”‚
â”‚  â”‚ â€¢ Load      â”‚  â”‚ â€¢ Compare   â”‚  â”‚ â€¢ Check     â”‚                â”‚
â”‚  â”‚   batches   â”‚  â”‚   stats     â”‚  â”‚   alerts    â”‚                â”‚
â”‚  â”‚ â€¢ Log       â”‚  â”‚ â€¢ KS test   â”‚  â”‚ â€¢ Call      â”‚                â”‚
â”‚  â”‚   metadata  â”‚  â”‚ â€¢ Mean/Std  â”‚  â”‚   GitHub    â”‚                â”‚
â”‚  â”‚             â”‚  â”‚ â€¢ Log       â”‚  â”‚   API       â”‚                â”‚
â”‚  â”‚             â”‚  â”‚   alerts    â”‚  â”‚             â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚         â”‚                â”‚                 â”‚                        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                          â”‚                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: MODEL TRAINING & EXPERIMENT TRACKING                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          â”‚                                           â”‚
â”‚                          â–¼                                           â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚              â”‚  GITHUB ACTIONS     â”‚                                â”‚
â”‚              â”‚  CI/CD/CT Pipeline  â”‚                                â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                         â”‚                                            â”‚
â”‚                         â–¼                                            â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚          â”‚  Training Workflow       â”‚                               â”‚
â”‚          â”‚  (.github/workflows/)    â”‚                               â”‚
â”‚          â”‚                          â”‚                               â”‚
â”‚          â”‚  1. Checkout code        â”‚                               â”‚
â”‚          â”‚  2. Setup Python         â”‚                               â”‚
â”‚          â”‚  3. Install deps         â”‚                               â”‚
â”‚          â”‚  4. Pull data from Neon  â”‚                               â”‚
â”‚          â”‚  5. Train model          â”‚                               â”‚
â”‚          â”‚  6. Log to MLflow        â”‚                               â”‚
â”‚          â”‚  7. Deploy if better     â”‚                               â”‚
â”‚          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                â”‚              â”‚                                      â”‚
â”‚                â–¼              â–¼                                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚    â”‚    NEON      â”‚    â”‚   MLFLOW     â”‚                            â”‚
â”‚    â”‚  (Read data) â”‚    â”‚  (Dagshub)   â”‚                            â”‚
â”‚    â”‚              â”‚    â”‚              â”‚                            â”‚
â”‚    â”‚ â€¢ ratings    â”‚    â”‚ â€¢ Experimentsâ”‚                            â”‚
â”‚    â”‚ â€¢ buffer     â”‚    â”‚ â€¢ Metrics    â”‚                            â”‚
â”‚    â”‚              â”‚    â”‚ â€¢ Models     â”‚                            â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ Artifacts  â”‚                            â”‚
â”‚                        â”‚ â€¢ Registry   â”‚                            â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 4: MODEL DEPLOYMENT & SERVING                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          â”‚                                           â”‚
â”‚                          â–¼                                           â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚              â”‚  DOCKER BUILD       â”‚                                â”‚
â”‚              â”‚  â€¢ FastAPI app      â”‚                                â”‚
â”‚              â”‚  â€¢ Load from MLflow â”‚                                â”‚
â”‚              â”‚  â€¢ Dependencies     â”‚                                â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                         â”‚                                            â”‚
â”‚                         â–¼                                            â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚              â”‚  DOCKER REGISTRY    â”‚                                â”‚
â”‚              â”‚  â€¢ Docker Hub       â”‚                                â”‚
â”‚              â”‚  â€¢ or GHCR          â”‚                                â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                         â”‚                                            â”‚
â”‚                         â–¼                                            â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚              â”‚  RENDER / RAILWAY   â”‚                                â”‚
â”‚              â”‚  Production Hosting â”‚                                â”‚
â”‚              â”‚                     â”‚                                â”‚
â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                                â”‚
â”‚              â”‚  â”‚   FASTAPI     â”‚ â”‚                                â”‚
â”‚              â”‚  â”‚               â”‚ â”‚                                â”‚
â”‚              â”‚  â”‚ Endpoints:    â”‚ â”‚                                â”‚
â”‚              â”‚  â”‚ /health       â”‚ â”‚                                â”‚
â”‚              â”‚  â”‚ /recommend    â”‚ â”‚                                â”‚
â”‚              â”‚  â”‚ /reload-model â”‚ â”‚                                â”‚
â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                                â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                         â”‚                                            â”‚
â”‚                         â–¼                                            â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚                   â”‚   USERS   â”‚                                     â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Component Details

### 1. Data Layer

#### Local Preprocessing Environment
**Location:** Developer's PC  
**Tools:** Python, Pandas, Jupyter Notebooks  
**Purpose:** Data preparation and initial processing

**Responsibilities:**
- Download MovieLens 25M dataset
- Exploratory Data Analysis (EDA)
- Data cleaning and validation
- Dataset reduction (25M â†’ 1M ratings)
- Train/test split (70/30)
- Buffer batch creation (weekly simulation)
- Feature engineering (movie embeddings)

**Outputs:**
- `ratings_initial_ml.parquet` (1M ratings)
- `buffer_batches/batch_wN.parquet` (5 batches)
- `movie_features_small.parquet` (movie metadata)

---

#### Neon PostgreSQL Database
**Type:** Serverless PostgreSQL  
**Tier:** Free (512 MB storage)  
**Region:** eu-central-1 (or user-selected)  
**Purpose:** Central data storage for MLOps pipeline

**Tables:**

1. **ratings** (~250 MB)
   - Training data (700k records)
   - Test data (300k records)
   - Split by `data_split` column

2. **movies** (~5 MB)
   - Movie metadata (60k movies)
   - Genre information
   - PCA feature embeddings

3. **ratings_buffer** (0-30 MB)
   - Staging area for new data
   - Accumulates weekly batches
   - Cleared after retraining

4. **ingestion_metadata** (<1 MB)
   - Batch ingestion logs
   - Record counts and timestamps
   - Status tracking

5. **model_metrics** (<1 MB)
   - Model performance history
   - RMSE, MAE, Precision@K
   - Training data size

6. **drift_alerts** (<1 MB)
   - Drift detection results
   - Statistical test outcomes
   - Alert triggers

**Access Patterns:**
- **Airflow:** Read/Write (ingestion, monitoring, logging)
- **Training Pipeline:** Read (model training data)
- **FastAPI:** Read (movie metadata, optional)

---

### 2. Orchestration Layer

#### Apache Airflow
**Deployment:** Local Docker Compose  
**Version:** 2.8+  
**Components:**
- Webserver (UI)
- Scheduler (DAG execution)
- PostgreSQL (metadata DB)
- Redis (optional, for CeleryExecutor)

**DAGs:**

**DAG 1: buffer_ingestion_weekly**
```python
Purpose: Load weekly batches into ratings_buffer
Schedule: Manual (or @weekly in production)
Tasks:
  1. ingest_batch_N - Load parquet file to Neon
  2. check_buffer_status - Verify ingestion
Dependencies: None (entry point)
```

**DAG 2: drift_monitoring**
```python
Purpose: Detect data distribution changes
Schedule: Manual (or @daily in production)
Tasks:
  1. load_baseline - Get training data stats
  2. load_new_data - Get buffer data stats
  3. calculate_drift - Run KS test, mean, std
  4. log_alerts - Write to drift_alerts table
  5. generate_report - Print recommendation
Dependencies: Requires data in ratings_buffer
```

**DAG 3: trigger_retraining** (Future)
```python
Purpose: Trigger model retraining via GitHub Actions
Schedule: Manual (or triggered by drift_monitoring)
Tasks:
  1. check_alerts - Query drift_alerts
  2. trigger_github - Call GitHub API
Dependencies: Requires alert_triggered = TRUE
```

**Configuration:**
- Environment variables in `.env`
- Volume mounts for DAGs and data
- Network access to Neon

---

### 3. Training Layer

#### GitHub Actions
**Platform:** GitHub-hosted runners  
**OS:** Ubuntu latest  
**Resources:** 2-core CPU, 7 GB RAM  

**Workflows:**

**model_training.yml**
```yaml
Triggers:
  - workflow_dispatch (manual)
  - repository_dispatch (from Airflow)
  
Steps:
  1. Checkout code
  2. Setup Python 3.11
  3. Install requirements (pandas, sklearn, mlflow)
  4. Connect to Neon (read data)
  5. Train collaborative filtering model
  6. Evaluate on test set
  7. Log to MLflow (metrics, model)
  8. Write metrics to model_metrics table
  9. Promote to production if improved
  
Environment:
  - NEON_CONNECTION_STRING (secret)
  - MLFLOW_TRACKING_URI (secret)
  - MLFLOW_USERNAME (secret)
  - MLFLOW_PASSWORD (secret)
```

**deploy_api.yml** (Future)
```yaml
Triggers:
  - workflow_run (after model_training succeeds)
  
Steps:
  1. Build Docker image with FastAPI
  2. Push to Docker Hub / GHCR
  3. Trigger Render deployment
  4. Run smoke tests
```

---

#### MLflow (Dagshub)
**Platform:** Dagshub (free tier)  
**Purpose:** Experiment tracking and model registry  

**Features Used:**
- **Experiment Tracking:**
  - Log hyperparameters
  - Log metrics (RMSE, MAE, Precision@K)
  - Log training duration
  
- **Artifact Storage:**
  - Trained model files
  - Training plots
  - Feature importances
  
- **Model Registry:**
  - Version models
  - Stage transitions (None â†’ Staging â†’ Production)
  - Model metadata

**Access:**
- **Training Script:** Write experiments and models
- **FastAPI:** Read production model
- **Team:** View experiments in web UI

---

### 4. Deployment Layer

#### FastAPI Application
**Framework:** FastAPI 0.109+  
**Python:** 3.11  
**Purpose:** Serve recommendations via REST API  

**Endpoints:**

```python
GET /health
  Returns: {"status": "healthy", "model_loaded": true}
  Purpose: Health check for deployment monitoring

POST /recommend
  Body: {"user_id": 123, "top_k": 10}
  Returns: {"user_id": 123, "recommendations": [...]}
  Purpose: Get movie recommendations for user

POST /reload-model
  Returns: {"status": "Model reloaded", "version": "v1.2"}
  Purpose: Manual model refresh from MLflow
```

**Startup:**
1. Load production model from MLflow
2. Cache model in memory
3. Start serving requests

**Model Loading:**
```python
import mlflow.pyfunc

model_uri = "models:/cinematch_recommender/Production"
model = mlflow.pyfunc.load_model(model_uri)
```

---

#### Docker Container
**Base Image:** python:3.11-slim  
**Size:** ~500 MB  

**Dockerfile Structure:**
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Environment Variables:**
- `MLFLOW_TRACKING_URI`
- `MLFLOW_USERNAME`
- `MLFLOW_PASSWORD`
- `NEON_CONNECTION_STRING` (optional, for logging)

---

#### Render / Railway
**Platform:** Render.com or Railway.app (free tier)  
**Purpose:** Cloud hosting for FastAPI  

**Configuration:**
- Auto-deploy from Docker image
- Environment variables via UI
- Custom domain support
- SSL/TLS automatic
- Health check monitoring

**Deployment Flow:**
1. GitHub Actions builds Docker image
2. Pushes to Docker Hub
3. Webhook triggers Render deployment
4. Render pulls new image
5. Rolling update (zero downtime)

---

## ğŸ”„ Data Flow

### End-to-End Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WEEK 1: INITIAL MONITORING CYCLE                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Monday:
  1. buffer_ingestion_dag runs
     â””â”€ Loads batch_w1.parquet â†’ Neon ratings_buffer
     â””â”€ Logs to ingestion_metadata
  
  2. drift_monitoring_dag runs
     â””â”€ Compares ratings (baseline) vs ratings_buffer (new)
     â””â”€ Calculates: KS=0.0176, Mean=0.97%, Std=0.27%
     â””â”€ Logs to drift_alerts: alert_triggered = FALSE
     â””â”€ Prints: "âœ… NO DRIFT - Continue monitoring"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WEEK 2: ACCUMULATING EVIDENCE                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Monday:
  1. buffer_ingestion_dag runs
     â””â”€ Loads batch_w3.parquet â†’ Neon ratings_buffer
     â””â”€ Total buffer: 86k ratings
  
  2. drift_monitoring_dag runs
     â””â”€ Compares ratings vs ratings_buffer (now 86k)
     â””â”€ Calculates: KS=0.042, Mean=6.8%, Std=2.3%
     â””â”€ Logs to drift_alerts: alert_triggered = FALSE
     â””â”€ Prints: "âš ï¸ Early drift signals - Continue monitoring"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WEEK 3: THRESHOLD EXCEEDED                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Monday:
  1. buffer_ingestion_dag runs
     â””â”€ Loads batch_w5.parquet â†’ Neon ratings_buffer
     â””â”€ Total buffer: 129k ratings
  
  2. drift_monitoring_dag runs
     â””â”€ Compares ratings vs ratings_buffer (now 129k)
     â””â”€ Calculates: KS=0.067, Mean=12.1%, Std=4.5%
     â””â”€ Logs to drift_alerts: alert_triggered = TRUE
     â””â”€ Prints: "ğŸš¨ DRIFT DETECTED - Retrain recommended"

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WEEK 4: RETRAINING & DEPLOYMENT                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Tuesday:
  1. trigger_retraining_dag runs
     â””â”€ Queries drift_alerts: Finds alert_triggered = TRUE
     â””â”€ Calls GitHub Actions API
     â””â”€ POST https://api.github.com/repos/.../dispatches
  
  2. GitHub Actions workflow starts
     â””â”€ Pulls data: ratings (train) + ratings_buffer
     â””â”€ Trains model: 700k + 129k = 829k ratings
     â””â”€ Evaluates: RMSE=0.82 (improved from 0.89)
     â””â”€ Logs to MLflow: run_id, metrics, model
     â””â”€ Writes to model_metrics table
     â””â”€ Promotes to "Production" stage in MLflow

Wednesday:
  3. Deployment workflow triggers
     â””â”€ Builds Docker image with FastAPI
     â””â”€ Pushes to Docker Hub: cinematch-api:v1.2
     â””â”€ Calls Render deploy hook
     â””â”€ Render pulls new image
     â””â”€ FastAPI loads model from MLflow Production
     â””â”€ New model serving recommendations

Thursday:
  4. Buffer cleanup
     â””â”€ Clear ratings_buffer (or merge to ratings)
     â””â”€ Ready for next monitoring cycle
```

---

## ğŸ”— Integration Points

### 1. Airflow â†” Neon
**Protocol:** PostgreSQL wire protocol  
**Connection:** SSL/TLS required  
**Authentication:** Username/password  

```python
from sqlalchemy import create_engine
import os

NEON_CONN = os.getenv('NEON_CONNECTION_STRING')
engine = create_engine(NEON_CONN)

# Read
df = pd.read_sql("SELECT * FROM ratings", engine)

# Write
df.to_sql('ratings_buffer', engine, if_exists='append')
```

**Credentials Storage:**
- Airflow `.env` file (local)
- Environment variables in container

---

### 2. Airflow â†’ GitHub Actions
**Protocol:** HTTPS REST API  
**Authentication:** Personal Access Token  

```python
import requests
import os

GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')

url = "https://api.github.com/repos/user/repo/actions/workflows/train.yml/dispatches"
headers = {
    'Authorization': f'token {GITHUB_TOKEN}',
    'Accept': 'application/vnd.github.v3+json'
}
data = {'ref': 'main', 'inputs': {'reason': 'drift_detected'}}

response = requests.post(url, headers=headers, json=data)
```

**Credentials:**
- GitHub PAT with `repo` and `workflow` scopes
- Stored in Airflow `.env`

---

### 3. GitHub Actions â†” Neon
**Protocol:** PostgreSQL wire protocol  
**Authentication:** Connection string in secrets  

```python
# In training script
import os
from sqlalchemy import create_engine

engine = create_engine(os.getenv('NEON_CONNECTION_STRING'))

# Pull training data
df_train = pd.read_sql("""
    SELECT * FROM ratings WHERE data_split = 'train'
    UNION ALL
    SELECT userId, movieId, rating, timestamp FROM ratings_buffer
""", engine)

# Log metrics
with engine.connect() as conn:
    conn.execute(f"""
        INSERT INTO model_metrics (model_version, rmse, mae)
        VALUES ('{run_id}', {rmse}, {mae})
    """)
```

**Credentials Storage:**
- GitHub repository secrets
- Injected as environment variables

---

### 4. GitHub Actions â†” MLflow
**Protocol:** HTTPS (MLflow REST API)  
**Authentication:** Username/password (Dagshub)  

```python
import mlflow
import os

mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI'))
os.environ['MLFLOW_TRACKING_USERNAME'] = os.getenv('MLFLOW_USERNAME')
os.environ['MLFLOW_TRACKING_PASSWORD'] = os.getenv('MLFLOW_PASSWORD')

with mlflow.start_run():
    mlflow.log_param('n_factors', 100)
    mlflow.log_metric('rmse', 0.82)
    mlflow.sklearn.log_model(model, "model")
```

**Credentials Storage:**
- GitHub repository secrets
- Dagshub account credentials

---

### 5. FastAPI â†” MLflow
**Protocol:** HTTPS (MLflow REST API)  
**Authentication:** Username/password  

```python
import mlflow.pyfunc
import os

# On startup
mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI'))
os.environ['MLFLOW_TRACKING_USERNAME'] = os.getenv('MLFLOW_USERNAME')
os.environ['MLFLOW_TRACKING_PASSWORD'] = os.getenv('MLFLOW_PASSWORD')

# Load production model
model = mlflow.pyfunc.load_model("models:/cinematch_recommender/Production")
```

**Credentials Storage:**
- Render environment variables
- Set via deployment UI

---

### 6. GitHub Actions â†’ Docker Hub â†’ Render
**Protocol:** Docker Registry API v2  
**Authentication:** Docker Hub credentials  

```yaml
# In deploy workflow
- name: Build and push
  run: |
    docker build -t user/cinematch-api:latest .
    echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
    docker push user/cinematch-api:latest

- name: Deploy to Render
  run: |
    curl -X POST ${{ secrets.RENDER_DEPLOY_HOOK }}
```

**Credentials Storage:**
- Docker Hub: GitHub secrets
- Render: Deploy hook URL in secrets

---

## ğŸ› ï¸ Tech Stack

### Data & Storage
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| Database | Neon PostgreSQL | 15 | Data storage |
| File Format | Apache Parquet | - | Local data files |
| Data Processing | Pandas | 2.1+ | Data manipulation |

### Orchestration
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| Workflow Engine | Apache Airflow | 2.8+ | Pipeline orchestration |
| Containerization | Docker | 24+ | Airflow deployment |
| Container Orchestration | Docker Compose | 2.23+ | Multi-container setup |

### Machine Learning
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| ML Library | scikit-surprise | 1.1+ | Collaborative filtering |
| Experiment Tracking | MLflow | 2.9+ | Model versioning |
| Statistical Tests | SciPy | 1.11+ | Drift detection |
| ML Platform | Dagshub | - | MLflow hosting |

### CI/CD
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| CI/CD Platform | GitHub Actions | - | Automated workflows |
| Version Control | Git | 2.40+ | Code versioning |

### Deployment
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| API Framework | FastAPI | 0.109+ | REST API |
| ASGI Server | Uvicorn | 0.27+ | Production server |
| Containerization | Docker | 24+ | API packaging |
| Hosting | Render/Railway | - | Cloud deployment |

### Development
| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| Language | Python | 3.11 | Primary language |
| Notebooks | Jupyter | - | EDA & prototyping |
| Environment | Conda/venv | - | Dependency isolation |

---

## ğŸš€ Deployment Architecture

### Development Environment
```
Developer's PC
â”œâ”€â”€ Airflow (Docker Compose)
â”‚   â”œâ”€â”€ Webserver: localhost:8080
â”‚   â”œâ”€â”€ Scheduler: Background
â”‚   â””â”€â”€ PostgreSQL: Airflow metadata
â”œâ”€â”€ Data Files
â”‚   â””â”€â”€ data/prepared/*.parquet
â””â”€â”€ Code Repository
    â””â”€â”€ Git â†’ GitHub
```

### Production Environment (Future)
```
Cloud Infrastructure
â”œâ”€â”€ Airflow (Managed)
â”‚   â”œâ”€â”€ AWS MWAA / GCP Composer
â”‚   â”œâ”€â”€ Autoscaling workers
â”‚   â””â”€â”€ S3/GCS for logs
â”œâ”€â”€ Database
â”‚   â”œâ”€â”€ Neon (upgraded tier)
â”‚   â””â”€â”€ or Supabase / Timescale
â”œâ”€â”€ Model Training
â”‚   â”œâ”€â”€ GitHub Actions
â”‚   â””â”€â”€ or Kubernetes Jobs
â”œâ”€â”€ Model Serving
â”‚   â”œâ”€â”€ Render / Railway
â”‚   â””â”€â”€ or AWS ECS / GCP Cloud Run
â””â”€â”€ Monitoring
    â”œâ”€â”€ Prometheus + Grafana
    â””â”€â”€ Slack/PagerDuty alerts
```

---

## ğŸ’¡ Design Decisions

### Architecture Patterns

#### 1. Lambda Architecture (Hybrid OLAP/OLTP)
**Decision:** Separate batch layer (Neon) and speed layer (potential Redis cache)

**Rationale:**
- Training = OLAP (batch, historical analysis)
- Serving = OLTP (fast lookups, low latency)

**Implementation:**
- Neon: Model training (batch reads)
- Redis (future): API serving (real-time reads)

---

#### 2. Event-Driven Orchestration
**Decision:** Airflow triggers GitHub Actions via API

**Rationale:**
- Loose coupling between components
- Each layer can scale independently
- Easy to swap implementations

**Alternative Considered:**
- Monolithic Airflow (all in one) â†’ Rejected (less modular)

---

#### 3. Microservices for Deployment
**Decision:** Separate services for training and serving

**Rationale:**
- Training: Compute-intensive, infrequent
- Serving: Lightweight, always-on
- Different scaling requirements

**Implementation:**
- Training: GitHub Actions (ephemeral)
- Serving: FastAPI (persistent)

---

#### 4. GitOps for ML
**Decision:** Code, configs, and triggers in Git

**Rationale:**
- Version control for reproducibility
- Code review for quality
- Easy rollback on failures

**Implementation:**
- DAGs versioned in Git
- Workflows versioned in Git
- Docker images tagged with Git SHA

---

### Technology Choices

#### Why Neon?
âœ… **Pros:**
- Serverless (auto-scaling)
- Free tier (512 MB)
- PostgreSQL (familiar SQL)
- Built-in backups

âŒ **Cons:**
- Storage limit (for academic project)
- Cold start latency

**Alternative:** Supabase, Timescale, managed PostgreSQL

---

#### Why Airflow?
âœ… **Pros:**
- Industry standard
- Rich UI
- Python-based (familiar)
- Active community

âŒ **Cons:**
- Heavy (needs Docker)
- Overkill for simple pipelines

**Alternative:** Prefect, Dagster, Cron + Python scripts

---

#### Why GitHub Actions?
âœ… **Pros:**
- Free for public repos
- Integrated with code
- Good for CI/CD
- Easy secrets management

âŒ **Cons:**
- Limited to 2-core machines
- No GPU support (free tier)

**Alternative:** GitLab CI, CircleCI, Jenkins

---

#### Why MLflow?
âœ… **Pros:**
- Open source
- Model registry
- Experiment tracking
- Cloud-agnostic

âŒ **Cons:**
- Basic UI
- No built-in deployment

**Alternative:** Weights & Biases, Neptune.ai, Comet.ml

---

#### Why FastAPI?
âœ… **Pros:**
- Modern, fast
- Auto documentation (Swagger)
- Type hints
- Async support

âŒ **Cons:**
- Relatively new

**Alternative:** Flask, Django REST

---

## ğŸ“Š System Characteristics

### Performance

**Ingestion Throughput:**
- 43k ratings in ~2-3 minutes
- ~250-300 records/second

**Drift Detection Latency:**
- Full analysis: ~5-7 minutes
- Statistical tests: ~30 seconds
- Logging overhead: ~30 seconds

**Model Training Time:**
- 700k ratings: ~5-10 minutes
- 829k ratings (w/ buffer): ~8-12 minutes

**API Response Time:**
- Health check: <50ms
- Recommendations: <200ms (cold start: ~2s)

---

### Scalability

**Current Limits:**
- Database: 512 MB (Neon free tier)
- Airflow: Single worker (local Docker)
- Training: 2-core CPU (GitHub Actions)
- API: 512 MB RAM (Render free tier)

**Production Scale (Estimated):**
- Database: 100+ GB (millions of ratings)
- Airflow: 10+ workers (managed service)
- Training: GPU instances (distributed training)
- API: Load balanced (multiple instances)

---

### Reliability

**Current:**
- Single point of failure (local Airflow)
- No automatic retries (configured in DAGs)
- Manual intervention for failures

**Production:**
- High availability (managed Airflow)
- Automatic retries (configured in DAGs)
- Health checks and alerts
- Rollback procedures

---

## ğŸ¯ Future Enhancements

### Near-Term (Next Month)
- [ ] Complete GitHub Actions integration
- [ ] Deploy FastAPI to Render
- [ ] Add deployment monitoring
- [ ] Implement A/B testing

### Medium-Term (3-6 Months)
- [ ] Upgrade to larger database
- [ ] Add Redis caching layer
- [ ] Implement real-time features
- [ ] Add Prometheus monitoring

### Long-Term (6-12 Months)
- [ ] Move to managed Airflow (MWAA)
- [ ] Implement feature store
- [ ] Add online learning
- [ ] Multi-model ensemble

---

## ğŸ“š References

**Architecture Patterns:**
- [Martin Fowler - Microservices](https://martinfowler.com/articles/microservices.html)
- [Lambda Architecture](http://lambda-architecture.net/)
- [MLOps Principles](https://ml-ops.org/)

**Tools Documentation:**
- [Airflow Docs](https://airflow.apache.org/docs/)
- [MLflow Docs](https://mlflow.org/docs/latest/index.html)
- [FastAPI Docs](https://fastapi.tiangolo.com/)

---

## ğŸ“ Key Takeaways

This architecture demonstrates:

âœ… **Modern MLOps practices** - CI/CD/CT/CM fully integrated  
âœ… **Separation of concerns** - Each layer independent  
âœ… **Cloud-native design** - Containerized, scalable  
âœ… **Production patterns** - Monitoring, logging, versioning  
âœ… **Cost-effective** - Utilizes free tiers effectively  
âœ… **Educational value** - Realistic professional architecture  

**Bottom Line:** This architecture balances learning objectives with production-grade practices, providing hands-on experience with industry-standard MLOps tools while remaining accessible for a bootcamp project.

---

**Last Updated:** December 18, 2025  
**Architecture Version:** 1.0  
**Maintained By:** CineMatch MLOps Team  
**Next Review:** After project completion
