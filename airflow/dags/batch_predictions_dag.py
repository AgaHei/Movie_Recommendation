"""
DAG Batch Predictions - DÃ©tection de Model Drift
=================================================
Cette DAG :
1. RÃ©cupÃ¨re un batch de donnÃ©es depuis ratings_buffer (via batch_id)
2. Fait des prÃ©dictions via l'API
3. Calcule les mÃ©triques (MAE, RMSE)
4. DÃ©clenche le rÃ©entrainement si performance dÃ©gradÃ©e
5. Sauvegarde les prÃ©dictions dans la BDD

DÃ©clenchÃ©e par buffer_ingestion_dag avec batch_id en configuration
"""

from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.operators.empty import EmptyOperator
from datetime import datetime, timedelta
import pandas as pd
import requests
from sqlalchemy import create_engine
import os
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

default_args = {
    'owner': 'cinematch',
    'depends_on_past': False,
    'email_on_failure': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
}

# Configuration API
API_URL = "https://julienrouillard-movie-recommendation-api.hf.space"

# Seuils de performance (dÃ©tection de drift)
EXPECTED_MAE = 0.79
EXPECTED_RMSE = 1.02
DEGRADATION_THRESHOLD = 0.10  # 10% de dÃ©gradation = rÃ©entrainement

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FONCTIONS HELPER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_db_engine():
    """CrÃ©e et retourne un engine SQLAlchemy pour Neon"""
    neon_conn = os.getenv('NEON_CONNECTION_STRING')
    if not neon_conn:
        raise ValueError("NEON_CONNECTION_STRING n'est pas dÃ©finie dans les variables d'environnement")
    return create_engine(neon_conn)


def load_batch_data(**context):
    """
    Charge les donnÃ©es du batch depuis ratings_buffer
    """
    print("\n" + "="*70)
    print("ğŸ“¥ CHARGEMENT DES DONNÃ‰ES DU BATCH")
    print("="*70)
    
    # RÃ©cupÃ©rer le batch_id depuis la config du DAG run
    batch_id = context['dag_run'].conf.get('batch_id') if context['dag_run'].conf else None
    
    if not batch_id:
        raise ValueError("batch_id manquant dans la configuration du DAG run")
    
    print(f"ğŸ¯ Batch ID: {batch_id}")
    
    # Connexion Ã  la BDD
    engine = get_db_engine()
    
    # Charger les donnÃ©es du batch
    query = """
        SELECT "userId", "movieId", "rating", "timestamp", "batch_id"
        FROM ratings_buffer
        WHERE batch_id = %(batch_id)s
        ORDER BY timestamp
    """
    
    df = pd.read_sql(query, engine, params={'batch_id': batch_id})
    
    if len(df) == 0:
        print(f"âš ï¸  Aucune donnÃ©e trouvÃ©e pour batch_id={batch_id}")
        context['ti'].xcom_push(key='batch_id', value=batch_id)
        context['ti'].xcom_push(key='data_empty', value=True)
        return
    
    print(f"âœ… {len(df):,} lignes chargÃ©es")
    print(f"   Users: {df['userId'].nunique()}")
    print(f"   Movies: {df['movieId'].nunique()}")
    print(f"   Rating moyen: {df['rating'].mean():.2f}")
    
    # Sauvegarder les donnÃ©es dans XCom
    context['ti'].xcom_push(key='batch_id', value=batch_id)
    context['ti'].xcom_push(key='batch_data', value=df.to_dict('records'))
    context['ti'].xcom_push(key='data_empty', value=False)
    
    print("="*70 + "\n")


def predict_batch(**context):
    """
    Appelle l'API pour faire des prÃ©dictions sur le batch (endpoint batch)
    """
    print("\n" + "="*70)
    print("ğŸ¤– PRÃ‰DICTIONS VIA API (BATCH ENDPOINT)")
    print("="*70)
    
    # VÃ©rifier si on a des donnÃ©es
    data_empty = context['ti'].xcom_pull(key='data_empty', task_ids='load_data')
    if data_empty:
        print("âš ï¸  Pas de donnÃ©es Ã  prÃ©dire")
        context['ti'].xcom_push(key='predictions_empty', value=True)
        return
    
    # RÃ©cupÃ©rer les donnÃ©es
    batch_data = context['ti'].xcom_pull(key='batch_data', task_ids='load_data')
    df = pd.DataFrame(batch_data)
    
    print(f"ğŸ“Š {len(df):,} prÃ©dictions Ã  faire")
    print(f"ğŸŒ API: {API_URL}/predict_batch")
    
    # PrÃ©parer les items pour l'appel batch
    items = [
        {"user_id": int(row['userId']), "movie_id": int(row['movieId'])}
        for _, row in df.iterrows()
    ]
    
    print(f"ğŸ“¦ Envoi de {len(items):,} items en batch...")
    
    try:
        # Appel API batch (un seul appel pour tout le batch !)
        response = requests.post(
            f"{API_URL}/predict_batch",
            json={"items": items},
            timeout=300  # 5 minutes de timeout pour les gros batches
        )
        
        if response.status_code == 200:
            api_predictions = response.json()['predictions']
            
            # CrÃ©er un dictionnaire pour mapper (userId, movieId) -> predicted_rating
            pred_map = {
                (pred['user_id'], pred['movie_id']): pred['rating']
                for pred in api_predictions
            }
            
            # Construire la liste finale avec actual_rating
            predictions = []
            for _, row in df.iterrows():
                user_id = int(row['userId'])
                movie_id = int(row['movieId'])
                predicted_rating = pred_map.get((user_id, movie_id))
                
                predictions.append({
                    'userId': user_id,
                    'movieId': movie_id,
                    'predicted_rating': predicted_rating,
                    'actual_rating': float(row['rating'])
                })
            
            success_count = len([p for p in predictions if p['predicted_rating'] is not None])
            error_count = len(predictions) - success_count
            
            print(f"\nâœ… PrÃ©dictions terminÃ©es")
            print(f"   SuccÃ¨s: {success_count:,}")
            print(f"   Erreurs: {error_count:,}")
            
        else:
            print(f"âŒ Erreur API: {response.status_code}")
            print(f"   Response: {response.text}")
            predictions = []
            
    except Exception as e:
        print(f"âŒ Exception lors de l'appel API batch: {e}")
        predictions = []
    
    # Sauvegarder les prÃ©dictions
    context['ti'].xcom_push(key='predictions', value=predictions)
    context['ti'].xcom_push(key='predictions_empty', value=(len(predictions) == 0))
    
    print("="*70 + "\n")


def calculate_metrics(**context):
    """
    Calcule les mÃ©triques MAE et RMSE pour dÃ©tecter le drift
    """
    print("\n" + "="*70)
    print("ğŸ“Š CALCUL DES MÃ‰TRIQUES (DÃ‰TECTION DRIFT)")
    print("="*70)
    
    # VÃ©rifier si on a des prÃ©dictions
    predictions_empty = context['ti'].xcom_pull(key='predictions_empty', task_ids='predict')
    if predictions_empty:
        print("âš ï¸  Pas de prÃ©dictions Ã  Ã©valuer")
        context['ti'].xcom_push(key='drift_detected', value=False)
        return
    
    # RÃ©cupÃ©rer les prÃ©dictions
    predictions = context['ti'].xcom_pull(key='predictions', task_ids='predict')
    batch_id = context['ti'].xcom_pull(key='batch_id', task_ids='load_data')
    
    n = len(predictions)
    print(f"ğŸ” Ã‰valuation de {n:,} prÃ©dictions (batch={batch_id})")
    
    # Calcul MAE et RMSE
    errors = [abs(p['predicted_rating'] - p['actual_rating']) for p in predictions]
    squared_errors = [(p['predicted_rating'] - p['actual_rating'])**2 for p in predictions]
    
    mae = np.mean(errors)
    rmse = np.sqrt(np.mean(squared_errors))
    
    print(f"\nğŸ“ˆ MÃ©triques actuelles:")
    print(f"   MAE:  {mae:.4f}")
    print(f"   RMSE: {rmse:.4f}")
    
    print(f"\nğŸ“ Baseline attendue:")
    print(f"   MAE:  {EXPECTED_MAE:.4f}")
    print(f"   RMSE: {EXPECTED_RMSE:.4f}")
    
    # Calcul de la dÃ©gradation
    mae_degradation = (mae / EXPECTED_MAE - 1) * 100
    rmse_degradation = (rmse / EXPECTED_RMSE - 1) * 100
    
    print(f"\nğŸ”¬ DÃ©gradation:")
    print(f"   MAE:  {mae_degradation:+.2f}%")
    print(f"   RMSE: {rmse_degradation:+.2f}%")
    print(f"   Seuil: {DEGRADATION_THRESHOLD*100:.0f}%")
    
    # DÃ©tection du drift
    mae_drift = mae > EXPECTED_MAE * (1 + DEGRADATION_THRESHOLD)
    rmse_drift = rmse > EXPECTED_RMSE * (1 + DEGRADATION_THRESHOLD)
    drift_detected = mae_drift or rmse_drift
    
    print(f"\n{'ğŸš¨ DRIFT DÃ‰TECTÃ‰!' if drift_detected else 'âœ… Pas de drift'}")
    
    if drift_detected:
        print(f"   {'MAE' if mae_drift else ''} {'RMSE' if rmse_drift else ''} dÃ©passe le seuil")
        print(f"   âš¡ Le rÃ©entrainement sera dÃ©clenchÃ©")
    else:
        print(f"   Performance stable, pas de rÃ©entrainement nÃ©cessaire")
    
    # Sauvegarder les rÃ©sultats
    context['ti'].xcom_push(key='mae', value=mae)
    context['ti'].xcom_push(key='rmse', value=rmse)
    context['ti'].xcom_push(key='drift_detected', value=bool(drift_detected))
    
    print("="*70 + "\n")


def save_predictions(**context):
    """
    Sauvegarde les prÃ©dictions dans la BDD (optimisÃ© avec batch updates)
    """
    print("\n" + "="*70)
    print("ğŸ’¾ SAUVEGARDE DES PRÃ‰DICTIONS DANS LA BDD (OPTIMISÃ‰)")
    print("="*70)
    
    # VÃ©rifier si on a des prÃ©dictions
    predictions_empty = context['ti'].xcom_pull(key='predictions_empty', task_ids='predict')
    if predictions_empty:
        print("âš ï¸  Pas de prÃ©dictions Ã  sauvegarder")
        return
    
    # RÃ©cupÃ©rer les prÃ©dictions
    predictions = context['ti'].xcom_pull(key='predictions', task_ids='predict')
    batch_id = context['ti'].xcom_pull(key='batch_id', task_ids='load_data')
    
    # Filtrer les prÃ©dictions valides (non-None)
    valid_predictions = [p for p in predictions if p['predicted_rating'] is not None]
    
    print(f"ğŸ’¾ Sauvegarde de {len(valid_predictions):,} prÃ©dictions (batch={batch_id})")
    
    if len(valid_predictions) == 0:
        print("âš ï¸  Aucune prÃ©diction valide Ã  sauvegarder")
        return
    
    # Connexion Ã  la BDD
    engine = get_db_engine()
    
    # CrÃ©er un DataFrame avec les prÃ©dictions
    pred_df = pd.DataFrame(valid_predictions)
    
    # CrÃ©er une table temporaire avec les prÃ©dictions
    with engine.begin() as conn:
        # CrÃ©er une table temp
        conn.execute("""
            CREATE TEMP TABLE IF NOT EXISTS temp_predictions (
                "userId" INT,
                "movieId" INT,
                predicted_rating FLOAT,
                batch_id TEXT
            )
        """)
        
        # InsÃ©rer toutes les prÃ©dictions dans la table temp
        pred_df[['userId', 'movieId', 'predicted_rating']].to_sql(
            'temp_predictions',
            conn,
            if_exists='append',
            index=False,
            method='multi'
        )
        
        # Ajouter le batch_id
        conn.execute(f"""
            UPDATE temp_predictions SET batch_id = '{batch_id}'
        """)
        
        # Faire le update en masse depuis la table temp vers ratings_buffer
        result = conn.execute("""
            UPDATE ratings_buffer rb
            SET predicted_rating = tp.predicted_rating
            FROM temp_predictions tp
            WHERE rb."userId" = tp."userId"
              AND rb."movieId" = tp."movieId"
              AND rb.batch_id = tp.batch_id
        """)
        
        updated = result.rowcount
    
    print(f"âœ… {updated:,} lignes mises Ã  jour dans ratings_buffer")
    print("="*70 + "\n")


def decide_retraining(**context):
    """
    DÃ©cide si on dÃ©clenche le rÃ©entrainement ou pas (branching)
    """
    print("\n" + "="*70)
    print("ğŸ” CHECKING IF RETRAINING NEEDED")
    print("="*70 + "\n")
    
    drift_detected = context['ti'].xcom_pull(key='drift_detected', task_ids='calculate_metrics')
    
    print(f"ğŸ” Valeur rÃ©cupÃ©rÃ©e de XCom: drift_detected = {drift_detected}")
    print(f"ğŸ” Type: {type(drift_detected)}")
    
    if drift_detected:
        print("ğŸ”¥ Drift dÃ©tectÃ© â†’ DÃ©clenchement du rÃ©entrainement")
        return 'trigger_test_retraining'
    else:
        print("âœ… Pas de drift â†’ Pas de rÃ©entrainement")
        return 'skip_retraining'


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with DAG(
    'batch_predictions',
    default_args=default_args,
    description='Batch predictions + dÃ©tection de drift + trigger rÃ©entrainement',
    schedule_interval=None,  # DÃ©clenchÃ© par buffer_ingestion_dag
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['batch', 'predictions', 'drift', 'cinematch'],
) as dag:
    
    # TÃ¢che 1: Charger les donnÃ©es du batch
    load_data = PythonOperator(
        task_id='load_data',
        python_callable=load_batch_data,
    )
    
    # TÃ¢che 2: Faire les prÃ©dictions via API
    predict = PythonOperator(
        task_id='predict',
        python_callable=predict_batch,
    )
    
    # TÃ¢che 3: Calculer les mÃ©triques
    calculate_metrics_task = PythonOperator(
        task_id='calculate_metrics',
        python_callable=calculate_metrics,
    )
    
    # TÃ¢che 4: Sauvegarder les prÃ©dictions dans la BDD
    save_predictions_task = PythonOperator(
        task_id='save_predictions',
        python_callable=save_predictions,
    )
    
    # TÃ¢che 5: DÃ©cider si on rÃ©entraine (branching)
    decide = BranchPythonOperator(
        task_id='decide_retraining',
        python_callable=decide_retraining,
    )
    
    # TÃ¢che 6a: DÃ©clencher le rÃ©entrainement (les tests)
    trigger_test_retraining = TriggerDagRunOperator(
        task_id='trigger_test_retraining',
        trigger_dag_id='ci_docker_build_test',
        wait_for_completion=False,
    )
    
    # TÃ¢che 6b: Ne rien faire (pas de rÃ©entrainement)
    skip_retraining = EmptyOperator(
        task_id='skip_retraining',
    )
    
    # TÃ¢che 7: Fin (pour reconverger les branches)
    end = EmptyOperator(
        task_id='end',
        trigger_rule='none_failed_min_one_success',
    )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DÃ‰PENDANCES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    load_data >> predict >> calculate_metrics_task >> save_predictions_task >> decide
    decide >> [trigger_test_retraining, skip_retraining]
    [trigger_test_retraining, skip_retraining] >> end